{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be optimize: Last get house may not be changed much.\n",
    "# Can save some intermediate scores to the database so that don't need to calculate all scores again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell contains methods that can be reused.\n",
    "def save_to_database(database, index_to_save):\n",
    "    pass\n",
    "\n",
    "def get_top_num(vec, num):\n",
    "    # assuming num is small, we use a O(n*num) solution instead of a full sort\n",
    "    index_arr = [] # stores the index we have get \n",
    "    print(vec)\n",
    "    for i in range(num):\n",
    "        current_best = -math.inf\n",
    "        current_best_index = 0\n",
    "        for j in range(len(vec)):\n",
    "            if(j not in index_arr):\n",
    "                if(vec[j] > current_best):\n",
    "                    current_best = vec[j]\n",
    "                    current_best_index = j\n",
    "        index_arr.append(current_best_index)\n",
    "    return index_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold start model\n",
    "# return this user's preference features and other users' features matrix\n",
    "def read_from_database_cold(user, database):\n",
    "    \n",
    "    pass\n",
    "\n",
    "# calculate cosine similarity between a vector and all columns of a matrix\n",
    "def get_similarity_cold(vec, mat):\n",
    "    dot = np.dot(mat,vec)\n",
    "    norm_product = np.linalg.norm(vec, ord = 2) * (np.sum(mat ** 2,axis=1)**(1./2))\n",
    "    norm_product = np.reshape(norm_product,(len(norm_product),1))\n",
    "    sim_vec = dot / norm_product\n",
    "    sim_vec = np.reshape(sim_vec,(len(sim_vec),))\n",
    "    return sim_vec\n",
    "        \n",
    "# main function for cold start model, num is the number of posts to be posted\n",
    "def get_house_cold(user, num):\n",
    "    database = \" \"\n",
    "    user, others = read_from_database_cold(user, database)\n",
    "    sim_vector = get_similarity_cold(user, others)\n",
    "    index_arr = get_top_num(sim_vector, num)\n",
    "    save_to_database(database, index_arr)\n",
    "\n",
    "def encoding():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87845528 1.         0.54791033]\n",
      "[1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# test for cold start, after encoding stage\n",
    "\n",
    "user_test_person_1 = np.array([0.3,0.2,0.95])  \n",
    "user_test_preference_1 = np.array([0.25,0.3,0.2])\n",
    "\n",
    "house_test_person_1 = np.array([0.2,0.4,0.7])\n",
    "house_test_preference_1 = np.array([0.1,0.7,0.3])\n",
    "\n",
    "house_test_person_2 = np.array([0.3,0.2,0.95])\n",
    "house_test_preference_2 = np.array([0.25,0.3,0.2])\n",
    "\n",
    "house_test_person_3 = np.array([0.1,0.7,0.2])\n",
    "house_test_preference_3 = np.array([0.7,0.8,0.9])\n",
    "\n",
    "user_1 = np.concatenate((user_test_person_1,user_test_preference_1))\n",
    "user_1 = np.reshape(user_1,(len(user_1),1))\n",
    "house_1 = np.concatenate((house_test_person_1,house_test_preference_1))\n",
    "house_2 = np.concatenate((house_test_person_2,house_test_preference_2))\n",
    "house_3 = np.concatenate((house_test_person_3,house_test_preference_3))\n",
    "house_mat = np.array([house_1,house_2,house_3])\n",
    "\n",
    "sim_vec = get_similarity_cold(user_1, house_mat)\n",
    "print(get_top_num(sim_vec,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for cold start\n",
    "user_test_person_1 = {\n",
    "    \"gender\": \"male\",\n",
    "    \"major\" : \"Computer Science\",\n",
    "    \"age\"   : 20,\n",
    "}\n",
    "\n",
    "user_test_preference_1 = {\n",
    "    'M':{\n",
    "    \"bedroom_want\" = 1,\n",
    "    \"payment_want\" = \"500-1000\",\n",
    "    \"laundary_want\" = \"in unit\",\n",
    "    \"parking_want\" = \"garage\",\n",
    "    \"other_feature_want\" = [\"hard wood\", \"dish washer\",\"air conditioning\"],\n",
    "    \"pets_want\" = \"dogs\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "house_test_1 = {\n",
    "    \n",
    "}\n",
    "house_test_2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CALCULATION MAY BE FASTER IF VECTORIZED. DEPENDS ON EXPERIMENT#\n",
    "\n",
    "# collaborative filter model\n",
    "# return this use and other uses as dictionary(ies).\n",
    "\n",
    "def read_from_database_collab(user, database):\n",
    "    pass\n",
    "    \n",
    "# this calculates this user and other users' similarity based on posts they viewed. These weights are used to weight the scores of houses\n",
    "def get_user_similarity_collab(vec,others):\n",
    "    scores = []\n",
    "    norm_user = np.linalg.norm(vec.values(), ord = 2)  # norm of this user\n",
    "    user_set = set(vec.keys())\n",
    "    for i in range(len(others)):\n",
    "        cur_dict = others[i]  # current dictionary, representation of the current other user being processed.\n",
    "        norm_cur = np.linalg.norm(cur_dict.values(),ord = 2)\n",
    "        \n",
    "        # get the common elements\n",
    "        cur_set = set(cur_dict.keys())\n",
    "        common = user_set & cur_set\n",
    "        \n",
    "        score = [vec[c]*cur_dict[c] for c in common]\n",
    "        score /= (norm_user * norm_cur)\n",
    "        scores.append(score)  # append current score to the scores list\n",
    "    return scores\n",
    "\n",
    "# main function for collaborative filter, num is the number of posts to be posted\n",
    "def get_house_collab(user, num):\n",
    "    database = \" \"\n",
    "    user, others = read_from_database_collab(user, database)\n",
    "    sim_vector = get_user_similarity_collab(user,others)\n",
    "    index_arr = get_top_num(sim_vector, num)\n",
    "    save_to_database(database,index_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have two models, Assume we have a threshold t, when user has pass through this threshold, we need to merge two models together\n",
    "# return the weight alpha and the time user viewed posts last time and\n",
    "# the posts information as a dict of list as values, ID as keys, which stores the time users view the post.\n",
    "#(Only posts get recommended last time). also, return who recommends these items \n",
    "\n",
    "def read_from_database_two(user, database):\n",
    "    return alpha, user_time, all_time, cold_recommend, collab_recommend\n",
    "\n",
    "def save_to_database_two(database, index_to_save, cold_recommend, collab_recommend, alpha):\n",
    "    pass\n",
    "\n",
    "# fit Gaussian to return the new alpha\n",
    "def update_alpha(alpha, user_time, all_time, cold_recommend, collab_recommend, learning_rate = 0.1):\n",
    "    # all_time is a dict with ID as the key and time as the values\n",
    "    keys = all_time.keys()\n",
    "    num_posts = len(keys)\n",
    "    p = np.zeros(len(keys))  # p stores the percentage of corresponding posts\n",
    "    \n",
    "    for i in range(num_posts):\n",
    "        cur_all_time = all_time[i]\n",
    "        cur_user_time = user_time[i]\n",
    "        \n",
    "        # compute the Gaussian parameters\n",
    "        mean = np.mean(cur_all_time)\n",
    "        var = np.var(cur_all_time)\n",
    "        p[i] = (cur_user_time - mean) / var\n",
    "    \n",
    "    cold_weight = 0\n",
    "    collab_weight = 0\n",
    "    cold_keys = cold\n",
    "    for i in range(len(p)):\n",
    "        if(keys[i] in cold_recommend):\n",
    "            cold_weight += p[i]\n",
    "        if(keys[i] in collab_recommend):\n",
    "            collab_weight += p[i]\n",
    "            \n",
    "    # Simply use the ratio for temp_alpha\n",
    "    temp_alpha = cold_weight / collab_weight\n",
    "    # interpolate to get the new alpha\n",
    "    new_alpha = alpha + learning_rate * (temp_alpha - alpha)\n",
    "    \n",
    "    return new_alpha\n",
    "        \n",
    "        \n",
    "# main function for merging two algorithms\n",
    "def get_house_two(user, num):\n",
    "    database = \" \"\n",
    "    alpha, user_time, all_time, cold_recommend, collab_recommend = read_from_database_two(user, database)\n",
    "    \n",
    "    # update alpha first and then use this alpha to recommend posts this time.\n",
    "    alpha = update_alpha(alpha,user_time,all_time, cold_recommend, collab_recommend)\n",
    "    \n",
    "    num_cold = int(alpha * num) + 1\n",
    "    num_collab = num - num_cold\n",
    "    \n",
    "    array_ind_1 = cold_get_house(user, num)\n",
    "    array_ind_2 = get_house_collab(user, num)\n",
    "    \n",
    "    array_ind = np.asarray(list(set(array_ind_1 + array_ind_2)))\n",
    "    np.random.shuffle(array_ind)\n",
    "    array_ind = array_ind[:num]\n",
    "    \n",
    "    save_to_database_two(database, array_ind, array_ind_1, array_ind_2, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell saved for matrix factorization. HOW TO APPLY MATRIX FACTORIZATION FOR ONLINE LEARNING???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community detection and  classification two-stage approach\n",
    "def read_from_database_community_clf(database):\n",
    "    pass\n",
    "\n",
    "def save_to_database_community_clf(database):\n",
    "    pass\n",
    "\n",
    "def community_update():\n",
    "    pass\n",
    "\n",
    "# update cv later\n",
    "def classification(X_train, y_train, X_test,clf = \"logistic\"):\n",
    "    clf = None\n",
    "    if(clf == \"logistic\"):\n",
    "        clf = LogisticRegression()\n",
    "    elif(clf == \"svm\"):\n",
    "        clf = LinearSVC()\n",
    "    elif(clf == \"gbm\"):\n",
    "        clf = LGBMClassifier()\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    model = CalibratedClassifierCV(clf, cv='prefit')\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.predict(X_test)\n",
    "\n",
    "def get_house_community_clf(database):\n",
    "    database = \"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
